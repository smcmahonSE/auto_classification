import argparse
from datetime import datetime, timezone
import json
from pathlib import Path

import joblib
import numpy as np
from lightgbm import LGBMClassifier
from sklearn.decomposition import PCA
from sklearn.metrics import (
    accuracy_score,
    balanced_accuracy_score,
    classification_report,
    precision_recall_fscore_support,
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


def parse_args():
    parser = argparse.ArgumentParser(
        description="Train LightGBM from cached embeddings and export joblib artifact."
    )
    parser.add_argument(
        "--features-path",
        default="artifacts/training_data/features_titan.npz",
        help="NPZ generated by build_training_data.py",
    )
    parser.add_argument("--output-dir", default="artifacts/model")
    parser.add_argument("--test-size", type=float, default=0.1)
    parser.add_argument("--random-state", type=int, default=42)
    parser.add_argument("--n-estimators", type=int, default=1000)
    parser.add_argument("--learning-rate", type=float, default=0.05)
    parser.add_argument("--num-leaves", type=int, default=64)
    parser.add_argument("--subsample", type=float, default=0.8)
    parser.add_argument("--colsample-bytree", type=float, default=0.8)
    parser.add_argument(
        "--experiment-name",
        default=None,
        help="Optional name for this run (used in metrics and history tracking).",
    )
    parser.add_argument(
        "--history-path",
        default="artifacts/model/metrics_history.jsonl",
        help="JSONL file where each run appends summary metrics for easy comparison.",
    )
    parser.add_argument(
        "--pca-components",
        type=int,
        default=None,
        help="Optional PCA dimensions before LightGBM (e.g. 256, 512).",
    )
    return parser.parse_args()


def file_size_mb(path: Path) -> float:
    return path.stat().st_size / (1024 * 1024)


def append_history_record(path: Path, record: dict) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(record) + "\n")


def main():
    args = parse_args()
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    artifact = np.load(args.features_path, allow_pickle=True)
    X = artifact["embeddings"].astype(np.float32)
    y_text = artifact["labels"].astype(str)

    # Guardrail: stratified splitting requires at least 2 rows per class.
    class_names_raw, class_counts_raw = np.unique(y_text, return_counts=True)
    valid_classes = class_names_raw[class_counts_raw >= 2]
    dropped_class_count = int(len(class_names_raw) - len(valid_classes))
    dropped_row_count = 0
    if dropped_class_count > 0:
        valid_mask = np.isin(y_text, valid_classes)
        dropped_row_count = int((~valid_mask).sum())
        X = X[valid_mask]
        y_text = y_text[valid_mask]
        print(
            "Warning: Dropped "
            f"{dropped_row_count} rows across {dropped_class_count} classes with <2 samples "
            "to satisfy train/test split requirements."
        )

    if len(np.unique(y_text)) < 2:
        raise ValueError(
            "Not enough classes to train after filtering sparse labels. "
            "Increase sample size or lower label sparsity before training."
        )

    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y_text)
    num_classes = len(label_encoder.classes_)

    split_strategy = "stratified"
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=args.test_size,
            stratify=y,
            random_state=args.random_state,
        )
    except ValueError as exc:
        # If class/sample geometry still causes split failures, continue with random split.
        split_strategy = "random_fallback"
        print(
            "Warning: Stratified split failed; falling back to random split. "
            f"Reason: {exc}"
        )
        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=args.test_size,
            stratify=None,
            random_state=args.random_state,
        )

    pca = None
    n_features_in = X_train.shape[1]
    if args.pca_components:
        if args.pca_components <= 0:
            raise ValueError("--pca-components must be > 0 when provided.")
        if args.pca_components >= n_features_in:
            print(
                f"Requested pca-components={args.pca_components} >= input dim={n_features_in}; skipping PCA."
            )
        else:
            print(f"Applying PCA: {n_features_in} -> {args.pca_components}")
            pca = PCA(n_components=args.pca_components, random_state=args.random_state)
            X_train = pca.fit_transform(X_train).astype(np.float32)
            X_test = pca.transform(X_test).astype(np.float32)

    model = LGBMClassifier(
        objective="multiclass",
        num_class=num_classes,
        learning_rate=args.learning_rate,
        num_leaves=args.num_leaves,
        max_depth=-1,
        subsample=args.subsample,
        colsample_bytree=args.colsample_bytree,
        n_estimators=args.n_estimators,
        class_weight="balanced",
        random_state=args.random_state,
        n_jobs=-1,
    )
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)
    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(
        y_test, y_pred, average="macro", zero_division=0
    )
    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(
        y_test, y_pred, average="weighted", zero_division=0
    )
    per_class_prf = precision_recall_fscore_support(
        y_test,
        y_pred,
        average=None,
        labels=np.arange(num_classes),
        zero_division=0,
    )

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Balanced accuracy: {balanced_accuracy:.4f}")
    print(
        "Macro P/R/F1: "
        f"{macro_precision:.4f} / {macro_recall:.4f} / {macro_f1:.4f}"
    )
    print(
        "Weighted P/R/F1: "
        f"{weighted_precision:.4f} / {weighted_recall:.4f} / {weighted_f1:.4f}"
    )
    print(
        classification_report(
            y_test,
            y_pred,
            labels=np.arange(num_classes),
            target_names=label_encoder.classes_,
            zero_division=0,
        )
    )

    model_input_dim = X_train.shape[1]
    model_bundle = {
        "model": model,
        "pca": pca,
        "label_encoder": label_encoder,
        "model_input_dim": int(model_input_dim),
        "raw_input_dim": int(X.shape[1]),
        "pca_components": int(args.pca_components) if pca is not None else None,
    }
    joblib_path = output_dir / "product_classifier.joblib"
    joblib.dump(model_bundle, joblib_path)
    print(f"Saved joblib model bundle: {joblib_path}")

    size_report = {
        "joblib_model_mb": round(file_size_mb(joblib_path), 3),
    }
    size_report["total_artifacts_mb"] = round(sum(size_report.values()), 3)
    # Simple deployment heuristic: 3x artifact size for runtime headroom.
    size_report["recommended_runtime_memory_mb"] = round(
        size_report["total_artifacts_mb"] * 3, 1
    )
    print("Artifact size report (MB):")
    for k, v in size_report.items():
        print(f"  {k}: {v}")

    class_names = [str(c) for c in label_encoder.classes_]
    per_class_recall = {
        class_names[i]: {
            "precision": float(per_class_prf[0][i]),
            "recall": float(per_class_prf[1][i]),
            "f1": float(per_class_prf[2][i]),
            "support": int(per_class_prf[3][i]),
        }
        for i in range(len(class_names))
    }

    metrics = {
        "run_timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "experiment_name": args.experiment_name,
        "accuracy": float(accuracy),
        "balanced_accuracy": float(balanced_accuracy),
        "macro_precision": float(macro_precision),
        "macro_recall": float(macro_recall),
        "macro_f1": float(macro_f1),
        "weighted_precision": float(weighted_precision),
        "weighted_recall": float(weighted_recall),
        "weighted_f1": float(weighted_f1),
        "test_size": args.test_size,
        "split_strategy": split_strategy,
        "dropped_sparse_class_count": dropped_class_count,
        "dropped_sparse_row_count": dropped_row_count,
        "random_state": args.random_state,
        "n_classes": int(num_classes),
        "n_features_raw": int(X.shape[1]),
        "n_features_model_input": int(model_input_dim),
        "pca_components": int(args.pca_components) if pca is not None else None,
        "model_artifact_path": str(joblib_path),
        "size_report_mb": size_report,
        "per_class_metrics": per_class_recall,
        "classification_report": classification_report(
            y_test,
            y_pred,
            labels=np.arange(num_classes),
            target_names=label_encoder.classes_,
            output_dict=True,
            zero_division=0,
        ),
    }
    metrics_path = output_dir / "metrics.json"
    with metrics_path.open("w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2)
    print(f"Saved metrics: {metrics_path}")

    history_record = {
        "run_timestamp_utc": metrics["run_timestamp_utc"],
        "experiment_name": args.experiment_name or output_dir.name,
        "metrics_path": str(metrics_path),
        "output_dir": str(output_dir),
        "accuracy": metrics["accuracy"],
        "balanced_accuracy": metrics["balanced_accuracy"],
        "macro_f1": metrics["macro_f1"],
        "weighted_f1": metrics["weighted_f1"],
        "pca_components": metrics["pca_components"],
        "n_estimators": args.n_estimators,
        "num_leaves": args.num_leaves,
        "learning_rate": args.learning_rate,
        "subsample": args.subsample,
        "colsample_bytree": args.colsample_bytree,
        "n_features_model_input": metrics["n_features_model_input"],
        "n_classes": metrics["n_classes"],
        "joblib_model_mb": metrics["size_report_mb"]["joblib_model_mb"],
        "total_artifacts_mb": metrics["size_report_mb"]["total_artifacts_mb"],
        "recommended_runtime_memory_mb": metrics["size_report_mb"][
            "recommended_runtime_memory_mb"
        ],
    }
    history_path = Path(args.history_path)
    append_history_record(history_path, history_record)
    print(f"Appended run summary to {history_path}")


if __name__ == "__main__":
    main()
